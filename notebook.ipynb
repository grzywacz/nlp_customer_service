{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Support Ticket Classification</h1>\n",
    "Please download the data from the below source:\n",
    "\n",
    "<a href=\"https://www.kaggle.com/code/aniketg11/support-tickets-classification/input\">Dataset</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Library for data manipulation and analysis\n",
    "import numpy as np # A fundamental package for scientific computing\n",
    "import re # Support for regular expressions\n",
    "import string # Support operations on strings\n",
    "import matplotlib.pyplot as plt # A plotting library that provides a MATLAB-like plotting framework\n",
    "import seaborn as sns # A Python data visualization library based on matplotlib\n",
    "from sklearn.model_selection import train_test_split # A function from the scikit-learn library used to split data arrays into two subsets: for training data and for testing data\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Part of scikit-learn's text processing module. Converts a collection of text documents to a matrix of token counts\n",
    "from sklearn.ensemble import RandomForestClassifier # A class that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve # This module includes score functions, performance metrics, and pairwise metrics and distance computations\n",
    "from nltk.corpus import stopwords # NLTK function to access lists of stop words for several languages.\n",
    "from nltk.tokenize import word_tokenize # NLTK function to split strings into tokens.\n",
    "from nltk.util import ngrams # NLTK function to generate n-grams from sequences of items.\n",
    "import nltk # Natural Language Toolkit (NLTK) is a leading platform for building programs to work with human language data.\n",
    "\n",
    "# Download NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE 1: Fill in the code to load and display the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASET\n",
    "# TODO: Read the downloaded dataset CSV file to pandas data frame using pandas read_csv function\n",
    "df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA EXPLORATION\n",
    "# Let's print out the full table. There is a 'body' column with the content of the message and 'ticket_type' column\n",
    "# with values 0 or 1, which is 0 = incident, 1= no_incident. Other columns are not used in this experiment.\n",
    "# TODO: Display the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING - DATA CLEAN UP\n",
    "In the preprocessing phase we will prepare the data to be properly consumed by the model. In this experiment we will apply the following (most common) techniques:\n",
    "1. Tokenization and lowercasing - splitting text into words and change all to lowercase\n",
    "2. Removing stopwords - Remove common words that do not contribute to the meaning\n",
    "3. Removing punctuation and special characters - clean the text by removing punctuation and special characters\n",
    "4. Bag of words/ Bigram character vectorization - convert the cleaned tokens into bigram characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE 2: Fill in the code for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # TODO: transform the text to lower case\n",
    "    text_lower_case = \n",
    "\n",
    "    # TODO: tokenize lower case version of the text using function imported form nltk package\n",
    "    tokens = \n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # TODO: remove stopwords from tokens\n",
    "    tokens = \n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    tokens = [re.sub(r'[^\\w\\s]', '', word) for word in tokens]\n",
    "    # Remove empty strings\n",
    "    tokens = [word for word in tokens if word]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "# TODO: apply the preprocessing function to the 'body' column and assign result to new processed_text column\n",
    "df['processed_text'] = \n",
    "\n",
    "df['processed_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE 3: Fill in the code to vectorize the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens2bigram(tokens):\n",
    "    # Function transforming tokens into bigram character ngrams\n",
    "    bigrams = list(ngrams(' '.join(tokens), 2))\n",
    "    return [''.join(bigram) for bigram in bigrams]\n",
    "\n",
    "# TODO: apply tokens2bigram function to processed_text column to and save result to new bigrams column\n",
    "df['bigrams'] = \n",
    "\n",
    "# Convert the list of bigrams to a string for CountVectorizer\n",
    "df['bigrams_str'] = df['bigrams'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "df['bigrams_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VECTORIZATION\n",
    "\n",
    "Convert the bigrams into vectors.\n",
    "\n",
    "<a href=\"https://www.kaggle.com/code/samuelcortinhas/nlp3-bag-of-words-and-similarity\">Useful article explaining 'Bag of Words' method </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize (create feature vetors) bigram strings using CountVectorizer from sklearn\n",
    "vectorizer = CountVectorizer()\n",
    "# TODO: use fit_transform method to produce feature matrix from data in the bigrams_str column\n",
    "X = \n",
    "\n",
    "# TODO: display the shape of the resulting feature matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET SPLIT\n",
    "\n",
    "Splitting dataset into training and testing set. In this experiment we will use 80/20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label\n",
    "y = df['ticket_type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINIG THE CLASSIFIER MODEL\n",
    "\n",
    "In this experiment we will use Random Forest Classifier.\n",
    "\n",
    "EXERCISE 4: Selecting Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternative models\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# model = MultinomialNB()\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Training the classifier\n",
    "model = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "\n",
    "# TODO: Fit the model on the training data\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION\n",
    "\n",
    "EXERCISE 5: Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Incident', 'No Incident'])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUROC evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUROC evaluation\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Plotting the ROC curve - Receiver Operating Characteristic curve\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
